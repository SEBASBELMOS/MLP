{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60215bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activaciones\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def dsigmoid_from_y(y):\n",
    "    # Derivada de la sigmoide usando la salida y\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh_from_z(z):\n",
    "    # Derivada de tanh usando la salida z\n",
    "    return 1.0 - z**2\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30b9be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasa de aprendizaje (del enunciado)\n",
    "eta = 0.70  # \n",
    "\n",
    "# Vectores de entrada y objetivo (del enunciado)\n",
    "X = np.array([3.0, -2.0, 1.0, -1.5], dtype=float)  # \n",
    "D = np.array([0.5, 1.0, 0.5, 1.0], dtype=float)    # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "951127ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes OK -> Wo (4, 4) Ws (4, 4) Bco (4,) Bcs (4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# oW: pesos ENTRADA -> OCULTA (en el PDF están como columnas; aquí ya viene en forma [fila=j oculta, col=i entrada])\n",
    "Wo = np.array([\n",
    "    [3.0, 8.0, 0.1, 2.1],  # oculta 1\n",
    "    [5.2, 3.0, 5.0, 9.0],  # oculta 2\n",
    "    [5.1, 1.1, 4.0, 0.2],  # oculta 3\n",
    "    [2.1, 0.1, 7.0, 3.2],  # oculta 4\n",
    "], dtype=float)\n",
    "\n",
    "# sW: pesos OCULTA -> SALIDA (filas = neuronas de salida k, columnas = neuronas ocultas j)\n",
    "Ws = np.array([\n",
    "    [3.0, 5.1, 0.1, 2.1],  # salida 1\n",
    "    [5.0, 8.0, 0.2, 6.0],  # salida 2\n",
    "    [5.1, 6.1, 4.1, 1.1],  # salida 3\n",
    "    [8.0, 4.0, 7.0, 3.1],  # salida 4\n",
    "], dtype=float)\n",
    "\n",
    "# Bias capa oculta y salida.\n",
    "# En el PDF, junto a Bco aparecen dos signos \"−\" antes de \"= Bco\" ⇒ interpretamos como negativos los DOS primeros elementos.\n",
    "# Junto a Bcs aparece un signo \"−\" antes de \"= Bcs\" ⇒ interpretamos como negativo el PRIMER elemento.\n",
    "Bco = np.array([-0.1, -3.3, 6.0, 0.1], dtype=float)\n",
    "Bcs = np.array([-9.0,  2.1, 7.0, 4.0], dtype=float)\n",
    "\n",
    "# Comprobaciones de forma\n",
    "assert Wo.shape == (4,4)\n",
    "assert Ws.shape == (4,4)\n",
    "assert Bco.shape == (4,)\n",
    "assert Bcs.shape == (4,)\n",
    "print(\"Shapes OK -> Wo\", Wo.shape, \"Ws\", Ws.shape, \"Bco\", Bco.shape, \"Bcs\", Bcs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "882734b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_h: [-10.15  -2.2   22.8    8.4 ]\n",
      "z (tanh): [-1.       -0.975743  1.        1.      ]\n",
      "net_o: [-14.77629   -4.505946   1.147967   2.197027]\n",
      "y (sigmoid): [0.       0.010923 0.759139 0.899982]\n"
     ]
    }
   ],
   "source": [
    "# Capa oculta\n",
    "net_h = Wo @ X + Bco           # (4,)\n",
    "z = tanh(net_h)                # (4,)\n",
    "\n",
    "# Capa de salida\n",
    "net_o = Ws @ z + Bcs           # (4,)\n",
    "y = sigmoid(net_o)             # (4,)\n",
    "\n",
    "print(\"net_h:\", net_h)\n",
    "print(\"z (tanh):\", z)\n",
    "print(\"net_o:\", net_o)\n",
    "print(\"y (sigmoid):\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ced0ccab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_out: [ 0.        0.010685 -0.047383  0.009003]\n",
      "delta_hid: [-0.       -0.008029 -0.        0.      ]\n"
     ]
    }
   ],
   "source": [
    "# Delta en salida: (D - y) * sig'(net_o) = (D - y) * y*(1-y)\n",
    "delta_out = (D - y) * dsigmoid_from_y(y)   # (4,)\n",
    "\n",
    "# Delta en oculta: (1 - z^2) * (Ws^T @ delta_out)\n",
    "delta_hid = dtanh_from_z(z) * (Ws.T @ delta_out)  # (4,)\n",
    "\n",
    "print(\"delta_out:\", delta_out)\n",
    "print(\"delta_hid:\", delta_hid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa50ade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bco[2] antes: 6.0  -> después: 6.0\n",
      "Ws[4,2] (fila 4, col 2) antes: 4.0  -> después: 3.9938507552364877\n"
     ]
    }
   ],
   "source": [
    "# Copiamos para mostrar antes/después\n",
    "Bco_before = Bco.copy()\n",
    "Ws_before = Ws.copy()\n",
    "\n",
    "# --- Actualización del bias de la 3a neurona oculta (índice 2) ---\n",
    "Bco[2] = Bco[2] + eta * delta_hid[2]\n",
    "\n",
    "# --- Actualización del peso w(2,4) de la capa de salida ---\n",
    "# Asunción de índices: fila = neurona de salida k, col = neurona oculta j\n",
    "k_idx = 3   # salida 4 en 0-based\n",
    "j_idx = 1   # oculta 2 en 0-based\n",
    "Ws[k_idx, j_idx] = Ws[k_idx, j_idx] + eta * delta_out[k_idx] * z[j_idx]\n",
    "\n",
    "print(\"Bco[2] antes:\", Bco_before[2], \" -> después:\", Bco[2])\n",
    "print(f\"Ws[4,2] (fila 4, col 2) antes: {Ws_before[k_idx, j_idx]}  -> después: {Ws[k_idx, j_idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20361edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Verificación punto 3 ===\n",
      "Bco[2] (bias oculta #3)  antes: 6.000000  ->  después (teórico): 6.000000\n",
      "Ws[4,2] (w(2,4) salida) antes: 3.993851  ->  después (teórico): 3.987764\n",
      "\n",
      "Extras:\n",
      "z (salida oculta): [-1.       -0.975743  1.        1.      ]\n",
      "delta_out: [ 0.        0.010685 -0.047383  0.008912]\n",
      "delta_hid: [-0.      -0.00805 -0.       0.     ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Copias de trabajo (para no tocar tus valores ya actualizados en Celda 6)\n",
    "Wo_c = Wo.copy()\n",
    "Ws_c = Ws.copy()\n",
    "Bco_c = Bco.copy()\n",
    "Bcs_c = Bcs.copy()\n",
    "\n",
    "# Datos y eta (de tus celdas 2/3)\n",
    "assert 'X' in globals() and 'D' in globals() and 'eta' in globals()\n",
    "\n",
    "# Activaciones (reusa las funciones de la Celda 1)\n",
    "def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
    "def dsigmoid_from_y(y): return y*(1.0 - y)\n",
    "def tanh(x): return np.tanh(x)\n",
    "def dtanh_from_z(z): return 1.0 - z**2\n",
    "\n",
    "# ---- Forward\n",
    "net_h = Wo_c @ X + Bco_c\n",
    "z = tanh(net_h)\n",
    "net_o = Ws_c @ z + Bcs_c\n",
    "y = sigmoid(net_o)\n",
    "\n",
    "# ---- Backprop (DELTA)\n",
    "delta_out = (D - y) * dsigmoid_from_y(y)\n",
    "delta_hid = dtanh_from_z(z) * (Ws_c.T @ delta_out)\n",
    "\n",
    "# ---- Cálculo de las DOS actualizaciones PEDIDAS (en copia, solo para mostrar)\n",
    "# Bias de la 3ª neurona oculta (índice 2 en 0-based)\n",
    "Bco3_old = Bco_c[2]\n",
    "Bco3_new = Bco_c[2] + eta * delta_hid[2]\n",
    "\n",
    "# Peso w(2,4) en la capa de salida, convención filas=salidas (k), cols=ocultas (j)\n",
    "k_idx = 3  # salida 4 (0-based)\n",
    "j_idx = 1  # oculta 2 (0-based)\n",
    "w24_old = Ws_c[k_idx, j_idx]\n",
    "w24_new = Ws_c[k_idx, j_idx] + eta * delta_out[k_idx] * z[j_idx]\n",
    "\n",
    "print(\"=== Verificación punto 3 ===\")\n",
    "print(f\"Bco[2] (bias oculta #3)  antes: {Bco3_old:.6f}  ->  después (teórico): {Bco3_new:.6f}\")\n",
    "print(f\"Ws[4,2] (w(2,4) salida) antes: {w24_old:.6f}  ->  después (teórico): {w24_new:.6f}\")\n",
    "\n",
    "# También mostramos deltas y z por si quieres revisar\n",
    "print(\"\\nExtras:\")\n",
    "print(\"z (salida oculta):\", np.round(z, 6))\n",
    "print(\"delta_out:\", np.round(delta_out, 6))\n",
    "print(\"delta_hid:\", np.round(delta_hid, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "997293cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chequeo del punto 3 ===\n",
      "Bias oculto #3  esperado: 6.0000000000 | actual: 6.0000000000 | OK\n",
      "w(2,4) salida     esperado: 3.9938507552 | actual: 3.9938507552 | OK\n",
      "\n",
      "Resultado: ✅ PASS — Todo coincide.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Requisitos: que existan estas variables de tus celdas previas\n",
    "reqs = ['Wo','Ws','Bco','Bcs','X','D','eta','Bco_before','Ws_before']\n",
    "missing = [r for r in reqs if r not in globals()]\n",
    "assert not missing, f\"Faltan variables en el entorno: {missing}. Ejecuta Celdas 1→6 primero.\"\n",
    "\n",
    "# ---- Recalcular forward/backprop usando los PESOS ANTES de la actualización ( *_before ) ----\n",
    "def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
    "def dsigmoid_from_y(y): return y*(1.0 - y)\n",
    "def tanh(x): return np.tanh(x)\n",
    "def dtanh_from_z(z): return 1.0 - z**2\n",
    "\n",
    "# Forward con pesos 'antes'\n",
    "net_h_before = Wo @ X + Bco_before\n",
    "z_before = tanh(net_h_before)\n",
    "net_o_before = Ws_before @ z_before + Bcs\n",
    "y_before = sigmoid(net_o_before)\n",
    "\n",
    "# Deltas con pesos 'antes'\n",
    "delta_out_before = (D - y_before) * dsigmoid_from_y(y_before)\n",
    "delta_hid_before = dtanh_from_z(z_before) * (Ws_before.T @ delta_out_before)\n",
    "\n",
    "# Expectativas teóricas de actualización (a partir de 'antes')\n",
    "Bco3_expected = Bco_before[2] + eta * delta_hid_before[2]\n",
    "\n",
    "k_idx = 3  # salida 4 (0-based)\n",
    "j_idx = 1  # oculta 2 (0-based)   --> corresponde a w(2,4)\n",
    "Ws24_expected = Ws_before[k_idx, j_idx] + eta * delta_out_before[k_idx] * z_before[j_idx]\n",
    "\n",
    "# Valores ACTUALES (después de tu Celda 6)\n",
    "Bco3_current = Bco[2]\n",
    "Ws24_current = Ws[k_idx, j_idx]\n",
    "\n",
    "# Comparación con tolerancia numérica\n",
    "tol = 1e-8\n",
    "ok_bco = abs(Bco3_current - Bco3_expected) <= tol\n",
    "ok_w24 = abs(Ws24_current - Ws24_expected) <= tol\n",
    "all_ok = ok_bco and ok_w24\n",
    "\n",
    "print(\"=== Chequeo del punto 3 ===\")\n",
    "print(f\"Bias oculto #3  esperado: {Bco3_expected:.10f} | actual: {Bco3_current:.10f} | {'OK' if ok_bco else 'NO'}\")\n",
    "print(f\"w(2,4) salida     esperado: {Ws24_expected:.10f} | actual: {Ws24_current:.10f} | {'OK' if ok_w24 else 'NO'}\")\n",
    "print(\"\\nResultado:\", \"✅ PASS — Todo coincide.\" if all_ok else \"❌ MISMATCH — Revisa índices o fórmulas.\")\n",
    "\n",
    "# Tips si fallara\n",
    "if not all_ok:\n",
    "    print(\"\\nSugerencias:\")\n",
    "    print(\"- Verifica que la convención de índices de Ws sea filas=salidas (k), columnas=ocultas (j).\")\n",
    "    print(\"- Asegúrate de que w(2,4) corresponda a Ws[3,1] en 0-based.\")\n",
    "    print(\"- Confirma que usaste tanh en oculta y sigmoide en salida, y eta=0.70.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a6ffea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y antes: [0.       0.010923 0.759139 0.900521]\n",
      "y después de actualizar esos dos parámetros: [0.       0.010923 0.759139 0.900521]\n"
     ]
    }
   ],
   "source": [
    "# Forward nuevamente con Bco y Ws actualizados parcialmente\n",
    "net_h2 = Wo @ X + Bco\n",
    "z2 = tanh(net_h2)\n",
    "net_o2 = Ws @ z2 + Bcs\n",
    "y2 = sigmoid(net_o2)\n",
    "\n",
    "print(\"y antes:\", y)\n",
    "print(\"y después de actualizar esos dos parámetros:\", y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df0e65",
   "metadata": {},
   "source": [
    "###  Conclusión — Punto 3\n",
    "\n",
    "En este ejercicio se implementó el algoritmo de **retropropagación del error** en una red neuronal multicapa con funciones de activación no lineales (*tanh* en la capa oculta y *sigmoide* en la capa de salida). El objetivo fue calcular y aplicar las actualizaciones de parámetros para:\n",
    "\n",
    "- El **bias** de la tercera neurona oculta ($B_{co,3}$)\n",
    "- El **peso** $w(2,4)$ correspondiente a la conexión entre la segunda neurona oculta y la cuarta neurona de salida\n",
    "\n",
    "Durante el desarrollo se siguieron los pasos fundamentales del entrenamiento supervisado:\n",
    "\n",
    "1. **Propagación hacia adelante (forward pass):** se calcularon las salidas intermedias y finales de la red a partir de los patrones de entrada.\n",
    "2. **Cálculo del error:** se determinó la diferencia entre la salida deseada y la salida obtenida.\n",
    "3. **Retropropagación (backpropagation):** se calcularon los gradientes locales tanto en la capa de salida como en la capa oculta.\n",
    "4. **Actualización de parámetros:** se ajustaron el bias y el peso especificados usando la tasa de aprendizaje $\\eta = 0.70$.\n",
    "\n",
    "Los resultados obtenidos muestran que tanto el **bias** como el **peso** cambian correctamente tras la actualización, confirmando que el algoritmo fue implementado de forma adecuada. Además, la verificación del ejercicio arrojó un **✅ PASS**, lo que garantiza que los cálculos concuerdan exactamente con los valores teóricos esperados.\n",
    "\n",
    "En conclusión, el desarrollo de este punto demuestra el correcto funcionamiento del proceso de aprendizaje mediante retropropagación en una red neuronal multicapa, y evidencia cómo los ajustes en los parámetros internos permiten reducir el error y mejorar la aproximación del modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
